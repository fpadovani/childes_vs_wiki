{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc98993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from utils.variables import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# Load spaCy tokenizers for English, French, and German\n",
    "nlp_dict = {\n",
    "    \"eng\": spacy.load(\"en_core_web_sm\"),\n",
    "    \"fra\": spacy.load(\"fr_core_news_sm\"),\n",
    "    \"deu\": spacy.load(\"de_core_news_sm\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_tokens_text(df, name):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a dataset and save the result to a CSV file.\n",
    "    \"\"\"\n",
    "    total_tokens = []\n",
    "\n",
    "    for sentence in df[\"text\"]:\n",
    "        if pd.isna(sentence):\n",
    "            continue\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        total_tokens.extend(tokens)\n",
    "    \n",
    "    print(f\"Total tokens in {name}: {len(total_tokens)}\")\n",
    "    return len(total_tokens)\n",
    "\n",
    "def counter_tokens_text_spacy(df, name, language):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a dataset and save the result to a CSV file.\n",
    "    \"\"\"\n",
    "    total_tokens = []\n",
    "    if language == 'eng':\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    elif language == 'fra':\n",
    "        nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "    else:\n",
    "        nlp = spacy.load(\"de_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "        \n",
    "    for sentence in df[\"text\"]:\n",
    "        doc = nlp(sentence)\n",
    "        sentence_tokens = [token.text for token in doc if not token.is_space and not token.is_punct]  # Exclude spaces and punctuation\n",
    "        total_tokens.extend(sentence_tokens)\n",
    "    \n",
    "    print(f\"Total tokens in {name}: {len(total_tokens)}\")\n",
    "    return len(total_tokens)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH\n",
      "Total tokens in train_eng_childes: 3942501\n",
      "Total tokens in valid_eng_childes: 321683\n",
      "Percentage of tokens in eval set compared to train set: 8.16%\n",
      "Total tokens in train_eng_wiki: 3923045\n",
      "Total tokens in valid_eng_wiki: 341139\n",
      "Percentage of tokens in eval set compared to train set: 8.70%\n"
     ]
    }
   ],
   "source": [
    "print('ENGLISH')\n",
    "train_eng_childes = pd.read_csv(TRAINING_CHILDES_ENG)\n",
    "valid_eng_childes = pd.read_csv(VALIDATION_CHILDES_ENG)\n",
    "\n",
    "\n",
    "train_eng_c = counter_tokens_text_spacy(train_eng_childes, \"train_eng_childes\", \"eng\")\n",
    "eval_eng_c = counter_tokens_text_spacy(valid_eng_childes, \"valid_eng_childes\",'eng')\n",
    "perc_childes_eng = (eval_eng_c/train_eng_c)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_childes_eng:.2f}%\")\n",
    "\n",
    "\n",
    "train_eng_wiki = pd.read_csv(TRAINING_WIKI_ENG)\n",
    "valid_eng_wiki = pd.read_csv(VALIDATION_WIKI_ENG)\n",
    "\n",
    "train_eng_w = counter_tokens_text_spacy(train_eng_wiki, \"train_eng_wiki\", 'eng')\n",
    "eval_eng_w = counter_tokens_text_spacy(valid_eng_wiki, \"valid_eng_wiki\", 'eng')\n",
    "perc_wiki_eng = (eval_eng_w/train_eng_w)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_wiki_eng:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718b7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in train and eval set: 4.2642M\n",
      "Total tokens in train and eval set: 4.2642M\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TOKENS_ENG_CHILDES = train_eng_c + eval_eng_c\n",
    "TOTAL_TOKENS_ENG_WIKI = train_eng_w + eval_eng_w\n",
    "\n",
    "TOTAL_TOKENS_ENG_CHILDES = TOTAL_TOKENS_ENG_CHILDES / 1_000_000\n",
    "TOTAL_TOKENS_ENG_WIKI = TOTAL_TOKENS_ENG_WIKI / 1_000_000\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_ENG_CHILDES:.4f}M\")\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_ENG_WIKI:.4f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FRENCH')\n",
    "train_fr_childes = pd.read_csv(TRAINING_CHILDES_FR)\n",
    "valid_fr_childes = pd.read_csv(VALIDATION_CHILDES_FR)\n",
    "\n",
    "train_fr_c = counter_tokens_text_spacy(train_fr_childes, \"train_fr_childes\", 'fra')\n",
    "eval_fr_c = counter_tokens_text_spacy(valid_fr_childes, \"valid_fr_childes\", 'fra')\n",
    "perc_childes_fr = (eval_fr_c/train_fr_c)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_childes_fr:.2f}%\")\n",
    "\n",
    "train_fr_wiki = pd.read_csv(TRAINING_WIKI_FR)\n",
    "valid_fr_wiki = pd.read_csv(VALIDATION_WIKI_FR)\n",
    "train_fr_w = counter_tokens_text_spacy(train_fr_wiki, \"train_fr_wiki\",'fra')\n",
    "eval_fr_w = counter_tokens_text_spacy(valid_fr_wiki, \"valid_fr_wiki\",'fra')\n",
    "perc_wiki_fr = (eval_fr_w/train_fr_w)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_wiki_fr:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TOKENS_FR_CHILDES = train_fr_c + eval_fr_c\n",
    "TOTAL_TOKENS_FR_WIKI = train_fr_w + eval_fr_w\n",
    "\n",
    "TOTAL_TOKENS_FR_CHILDES = TOTAL_TOKENS_FR_CHILDES / 1_000_000\n",
    "TOTAL_TOKENS_FR_WIKI = TOTAL_TOKENS_FR_WIKI / 1_000_000\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_FR_CHILDES:.2f}M\")\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_FR_WIKI:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5658c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GERMAN')\n",
    "train_de_childes = pd.read_csv(TRAINING_CHILDES_DE)\n",
    "valid_de_childes = pd.read_csv(VALIDATION_CHILDES_DE)\n",
    "\n",
    "train_de_c = counter_tokens_text_spacy(train_de_childes, \"train_de_childes\", 'deu')\n",
    "eval_de_c = counter_tokens_text_spacy(valid_de_childes, \"valid_de_childes\", 'deu')\n",
    "perc_childes_de = (eval_de_c/train_de_c)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_childes_de:.2f}%\")\n",
    "\n",
    "train_de_wiki = pd.read_csv(TRAINING_WIKI_DE)\n",
    "valid_de_wiki = pd.read_csv(VALIDATION_WIKI_DE)\n",
    "train_de_w = counter_tokens_text_spacy(train_de_wiki, \"train_de_wiki\", 'deu')\n",
    "eval_de_w = counter_tokens_text_spacy(valid_de_wiki, \"valid_de_wiki\", 'deu')\n",
    "perc_wiki_de = (eval_de_w/train_de_w)*100\n",
    "print(f\"Percentage of tokens in eval set compared to train set: {perc_wiki_de:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TOKENS_DE_CHILDES = train_de_c + eval_de_c\n",
    "TOTAL_TOKENS_DE_WIKI = eval_de_w + train_de_w\n",
    "\n",
    "TOTAL_TOKENS_DE_CHILDES = TOTAL_TOKENS_DE_CHILDES / 1_000_000\n",
    "TOTAL_TOKENS_DE_WIKI = TOTAL_TOKENS_DE_WIKI / 1_000_000\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_DE_CHILDES:.2f}M\")\n",
    "print(f\"Total tokens in train and eval set: {TOTAL_TOKENS_DE_WIKI:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8d483",
   "metadata": {},
   "source": [
    "### TOTAL TOKENS IN THE DATASETS overall \n",
    "The values below are those reported in the paper (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "     \"AO_CHILDES_ENG\": AO_CHILDES_ENGLISH,\n",
    "    \"AO_CHILDES_FR\": AO_CHILDES_FRENCH,\n",
    "    \"AO_CHILDES_DE\": AO_CHILDES_GERMAN,\n",
    "    \"WIKIPEDIA_ENG\": WIKIPEDIA_ENG,\n",
    "    \"WIKIPEDIA_FR\": WIKIPEDIA_FR,\n",
    "    \"WIKIPEDIA_DE\": WIKIPEDIA_DE,\n",
    "}\n",
    "\n",
    "def ngram_ratio(file_path,name, n):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    tokens_all = []\n",
    "    for sentence in df['sentences']:\n",
    "        if pd.isna(sentence):\n",
    "            continue\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "        tokens_all.extend(tokens)\n",
    "\n",
    "    ngs = list(ngrams(tokens_all, n))\n",
    "    total = len(ngs)\n",
    "    unique = len(set(ngs))\n",
    "    \n",
    "    return unique / total if total > 0 else 0\n",
    "    \n",
    "\n",
    "for name, path in datasets.items():\n",
    "    print(f\"\\nAnalyzing: {name}\")\n",
    "    unigram_ratio = ngram_ratio(path,name, 1)\n",
    "    bigram_ratio = ngram_ratio(path,name, 2)\n",
    "    trigram_ratio = ngram_ratio(path,name, 3)\n",
    "    \n",
    "    print(f\"Unigram Ratio: {unigram_ratio:.3f}\")\n",
    "    print(f\"Bigram Ratio: {bigram_ratio:.3f}\")\n",
    "    print(f\"Trigram Ratio: {trigram_ratio:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8c312",
   "metadata": {},
   "source": [
    "Different ways of computing token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca678d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def tokenize_text(text, lang=\"eng\"):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on the specified language.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input utterance.\n",
    "        lang (str): Language code ('eng', 'fra', 'deu').\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): List of tokens.\n",
    "    \"\"\"\n",
    "    if lang not in nlp_dict:\n",
    "        raise ValueError(f\"Unsupported language: {lang}. Choose from 'eng', 'fra', 'deu'.\")\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_transcripts(transcripts, lang=\"eng\"):\n",
    "    \"\"\"\n",
    "    Processes a dataset of utterances by tokenizing and counting tokens.\n",
    "\n",
    "    Parameters:\n",
    "        transcripts (list of str): List of utterances.\n",
    "        lang (str): Language code ('eng', 'fra', 'deu').\n",
    "\n",
    "    Returns:\n",
    "        dict: Total tokens, unique tokens, and token frequency.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    \n",
    "    for utterance in transcripts:\n",
    "        tokens = tokenize_text(utterance, lang)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    total_tokens = len(all_tokens)\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"unique_tokens\": unique_tokens,\n",
    "        \"unigram_freq\": token_freq\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a71fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def compute_ttr(sentences, lang=\"eng\", sample_size=10000, num_samples=10):\n",
    "    \"\"\"\n",
    "    Computes Type-Token Ratio (TTR) by sampling 10,000 sentences 10 times and averaging TTR scores.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): List of utterances.\n",
    "        lang (str): Language code ('eng', 'fra', 'deu').\n",
    "        sample_size (int): Number of sentences to sample per run.\n",
    "        num_samples (int): Number of sampling iterations.\n",
    "\n",
    "    Returns:\n",
    "        float: Average TTR across 10 samples.\n",
    "    \"\"\"\n",
    "    if len(sentences) < sample_size:\n",
    "        raise ValueError(\"Not enough sentences in dataset. Reduce sample_size or provide more data.\")\n",
    "    \n",
    "    ttr_values = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_sentences = random.sample(sentences, sample_size)\n",
    "        all_tokens = []\n",
    "\n",
    "        for sentence in sampled_sentences:\n",
    "            tokens = tokenize_text(sentence, lang)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        total_tokens = len(all_tokens)\n",
    "        unique_tokens = len(set(all_tokens))\n",
    "        ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "        ttr_values.append(ttr)\n",
    "\n",
    "    avg_ttr = np.mean(ttr_values)\n",
    "    return avg_ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_questions(sentences):\n",
    "    \n",
    "    # Filter sentences that end with a question mark\n",
    "    question_count = sum(1 for sentence in sentences if sentence.strip().endswith('?'))\n",
    "    \n",
    "    return question_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(sentences, lang=\"eng\"):\n",
    "    \"\"\"\n",
    "    Computes the average sentence length in terms of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): List of utterances/sentences.\n",
    "        lang (str): Language code ('eng', 'fra', 'deu').\n",
    "    \n",
    "    Returns:\n",
    "        float: The average sentence length in tokens.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize_text(sentence, lang)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "    avg_length = total_tokens / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_length\n",
    "\n",
    "def compute_sentence_lengths(sentences):\n",
    "    \"\"\"\n",
    "    Computes the number of words in each sentence using simple whitespace splitting.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): List of utterances/sentences.\n",
    "    \n",
    "    Returns:\n",
    "        list of int: List containing the length of each sentence.\n",
    "    \"\"\"\n",
    "    return [len(sentence.strip().split()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b61b4",
   "metadata": {},
   "source": [
    "### ENGLISH CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_eng = pd.read_csv(AO_CHILDES_ENGLISH)\n",
    "transcripts_eng_childes = childes_eng['sentences'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_eng[\"sentence_length\"] = compute_sentence_lengths(childes_eng[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_eng_childes = process_transcripts(transcripts_eng_childes, lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years_of_life = childes_eng['year_of_life'].nunique()\n",
    "print(f\"Unique years of life: {unique_years_of_life}\")\n",
    "\n",
    "# Unique transcript_ids\n",
    "unique_transcript_ids_list = childes_eng['transcript_id'].nunique()\n",
    "print(f\"Unique transcript_id values: {unique_transcript_ids_list}\")\n",
    "\n",
    "# Unique age_in_days values\n",
    "unique_age_in_days_list = childes_eng['age_in_days'].nunique()\n",
    "print(f\"Unique age_in_days values: {unique_age_in_days_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unigram_freq_eng = pd.DataFrame(results_eng_childes['unigram_freq'].items(), columns=['token', 'freq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_eng = compute_ttr(transcripts_eng_childes, lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbe4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_questions = count_questions(transcripts_eng_childes)\n",
    "print((eng_questions/len(transcripts_eng_childes))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acefeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_avg_sent_length = average_sentence_length(transcripts_eng_childes, lang=\"eng\")\n",
    "eng_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed960",
   "metadata": {},
   "source": [
    "### GERMAN CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c60ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_deu = pd.read_csv(AO_CHILDES_GERMAN)\n",
    "transcripts_deu_childes = childes_deu['sentences'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0fbcd",
   "metadata": {},
   "source": [
    "Given that the encoding of the age in days is wrong, we apply this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7746735",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_deu.rename(columns={'age_in_days': 'months'}, inplace=True)\n",
    "childes_deu['year_of_life_fine_grained'] = round(childes_deu['months'] / 12,2)\n",
    "childes_deu['year_of_life'] = childes_deu['year_of_life_fine_grained'].astype(int)\n",
    "childes_deu['age_in_days'] = childes_deu['months'] * 30.44\n",
    "childes_deu.drop(columns=['age_in_months'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfeea9",
   "metadata": {},
   "source": [
    "childes_deu['sentence_length'] = compute_sentence_lengths(childes_deu[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56665b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deu_childes = process_transcripts(transcripts_deu_childes, lang=\"deu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_deu= pd.DataFrame(results_deu_childes['unigram_freq'].items(), columns=['token', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9878b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years_of_life = childes_deu['year_of_life'].nunique()\n",
    "print(f\"Unique years of life: {unique_years_of_life}\")\n",
    "\n",
    "# Unique transcript_ids\n",
    "unique_transcript_ids_list = childes_deu['transcript_id'].nunique()\n",
    "print(f\"Unique transcript_id values: {unique_transcript_ids_list}\")\n",
    "\n",
    "# Unique age_in_days values\n",
    "unique_age_in_days_list = childes_deu['age_in_days'].nunique()\n",
    "print(f\"Unique age_in_days values: {unique_age_in_days_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_deu = compute_ttr(transcripts_deu_childes, lang=\"deu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"German TTR: {ttr_deu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_questions = count_questions(transcripts_deu_childes)\n",
    "print((deu_questions/len(transcripts_deu_childes))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ce4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_avg_sent_length = average_sentence_length(transcripts_deu_childes, lang=\"deu\")\n",
    "deu_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce51cb8",
   "metadata": {},
   "source": [
    "### FRENCH CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_fra = pd.read_csv(AO_CHILDES_FRENCH)\n",
    "transcripts_fra_childes = childes_fra['sentences'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf23a2",
   "metadata": {},
   "source": [
    "Given that the encoding of the age in days is wrong, we apply this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36006f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_fra.rename(columns={'age_in_days': 'months'}, inplace=True)\n",
    "childes_fra['year_of_life_fine_grained'] = round(childes_fra['months'] / 12, 2)\n",
    "childes_fra['year_of_life'] = childes_fra['year_of_life_fine_grained'].astype(int)\n",
    "childes_fra['age_in_days'] = childes_fra['months'] * 30.44\n",
    "childes_fra.drop(columns=['age_in_months'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca422e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_fra['sentence_length'] = compute_sentence_lengths(childes_fra[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa19003",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fra_childes = process_transcripts(transcripts_fra_childes, lang=\"fra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_fra = pd.DataFrame(results_fra_childes['unigram_freq'].items(), columns=['token', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_fra = compute_ttr(transcripts_fra_childes, lang=\"fra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years_of_life = childes_fra['year_of_life'].nunique()\n",
    "print(f\"Unique years of life: {unique_years_of_life}\")\n",
    "\n",
    "# Unique transcript_ids\n",
    "unique_transcript_ids_list = childes_fra['transcript_id'].nunique()\n",
    "print(f\"Unique transcript_id values: {unique_transcript_ids_list}\")\n",
    "\n",
    "# Unique age_in_days values\n",
    "unique_age_in_days_list = childes_fra['age_in_days'].nunique()\n",
    "print(f\"Unique age_in_days values: {unique_age_in_days_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35907928",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_questions = count_questions(transcripts_fra_childes)\n",
    "print((fra_questions/len(transcripts_fra_childes))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84caff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_avg_sent_length = average_sentence_length(transcripts_fra_childes, lang=\"fra\")\n",
    "fra_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdce10",
   "metadata": {},
   "source": [
    "### ENGLISH WIKIPEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_eng = pd.read_csv(WIKIPEDIA_ENG)\n",
    "transcripts_eng_wiki = wiki_eng['sentences'].tolist()\n",
    "\n",
    "wiki_eng['sentence_length'] = compute_sentence_lengths(wiki_eng[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_eng_wiki = process_transcripts(transcripts_eng_wiki, lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27fdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_eng = pd.DataFrame(results_eng_wiki['unigram_freq'].items(), columns=['token', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce00df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_eng_wiki = compute_ttr(transcripts_eng_wiki, lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8603a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"English TTR Wiki: {ttr_eng_wiki:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04baeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_questions_wiki = count_questions(transcripts_eng_wiki)\n",
    "print(eng_questions_wiki)\n",
    "print((eng_questions_wiki/len(transcripts_eng_wiki))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_avg_sent_length = average_sentence_length(transcripts_eng_wiki, lang=\"eng\")\n",
    "eng_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923a8b2",
   "metadata": {},
   "source": [
    "### GERMAN WIKIPEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_deu = pd.read_csv(WIKIPEDIA_DE)\n",
    "transcripts_deu_wiki = wiki_deu['sentences'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90355813",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_deu['sentence_length'] = compute_sentence_lengths(wiki_deu[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deu_wiki = process_transcripts(transcripts_deu_wiki, lang=\"deu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_deu = pd.DataFrame(results_deu_wiki['unigram_freq'].items(), columns=['token', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_deu_wiki = compute_ttr(transcripts_deu_wiki, lang=\"deu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90150400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"German TTR Wiki: {ttr_deu_wiki:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90718e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_questions_wiki = count_questions(transcripts_deu_wiki)\n",
    "print(deu_questions_wiki)\n",
    "print((deu_questions_wiki/len(transcripts_deu_wiki))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_avg_sent_length = average_sentence_length(transcripts_deu_wiki, lang=\"deu\")\n",
    "deu_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954d957",
   "metadata": {},
   "source": [
    "### FRENCH WIKIPEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_fra = pd.read_csv(WIKIPEDIA_FR)\n",
    "transcripts_fra_wiki = wiki_fra['sentences'].tolist()\n",
    "\n",
    "wiki_fra['sentence_length'] = compute_sentence_lengths(wiki_fra[\"sentences\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fra_wiki = process_transcripts(transcripts_fra_wiki, lang=\"fra\")\n",
    "results_fra_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_fra = pd.DataFrame(results_fra_wiki['unigram_freq'].items(), columns=['token', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_fra_wiki = compute_ttr(transcripts_fra_wiki, lang=\"fra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"French TTR Wiki: {ttr_fra_wiki:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_questions_wiki = count_questions(transcripts_fra_wiki)\n",
    "print(fra_questions_wiki)\n",
    "print((fra_questions_wiki/len(transcripts_fra_wiki))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b01025",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_avg_sent_length = average_sentence_length(transcripts_fra_wiki, lang=\"fra\")\n",
    "fra_avg_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b886e",
   "metadata": {},
   "source": [
    "### PLOT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554568b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your bins and labels\n",
    "bins = [0, 3, 6, 12, 200]\n",
    "bin_labels = ['1–3', '4–6', '7–12', '13–200']\n",
    "\n",
    "def prepare_binned_counts(df, domain_name):\n",
    "    \"\"\"\n",
    "    Bins sentence lengths and returns bin counts for plotting.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must include 'sentence_length' column.\n",
    "        domain_name (str): Either 'Childes' or 'Wikipedia'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Counts of sentences per bin.\n",
    "    \"\"\"\n",
    "    binned = pd.cut(df['sentence_length'], bins=bins, labels=bin_labels, right=True)\n",
    "    bin_counts = binned.value_counts().sort_index()\n",
    "    return pd.DataFrame({\n",
    "        'Length Bin': bin_labels,\n",
    "        'Count': bin_counts.values,\n",
    "        'Domain': domain_name\n",
    "    })\n",
    "\n",
    "\n",
    "domain_color_map = {\n",
    "    \"CHILDES\": \"#e74c3c\", \n",
    "    \"Wikipedia\": \"#f1c40f\"  \n",
    "}\n",
    "\n",
    "# English \n",
    "combined_eng = pd.concat([\n",
    "    prepare_binned_counts(childes_eng, \"CHILDES\"),\n",
    "    prepare_binned_counts(wiki_eng, \"Wikipedia\")\n",
    "])\n",
    "\n",
    "# French\n",
    "combined_fra = pd.concat([\n",
    "    prepare_binned_counts(childes_fra, \"CHILDES\"),\n",
    "    prepare_binned_counts(wiki_fra, \"Wikipedia\")\n",
    "])\n",
    "# German\n",
    "combined_deu = pd.concat([\n",
    "    prepare_binned_counts(childes_deu, \"CHILDES\"),\n",
    "    prepare_binned_counts(wiki_deu, \"Wikipedia\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = [combined_eng, combined_fra, combined_deu]\n",
    "language_titles = ['English', 'French', 'German']\n",
    "global_max = max(df['Count'].max() for df in combined_data)\n",
    "\n",
    "width_in = 6.875  # or 3.25 for 1-column width\n",
    "height_in = 4.0\n",
    "dpi = 300\n",
    "\n",
    "width_px = int(width_in * dpi)\n",
    "height_px = int(height_in * dpi)\n",
    "\n",
    "# Set y-axis max\n",
    "y_max = math.ceil(global_max / 20000) * 20000 \n",
    "\n",
    "# Create 3-column subplot\n",
    "fig = make_subplots(rows=1, cols=3, shared_yaxes=True,\n",
    "                    subplot_titles=[f\"{lang}\" for lang in language_titles])\n",
    "\n",
    "# Plot each language's grouped bar chart\n",
    "for i, data in enumerate(combined_data):\n",
    "    col = i + 1  # subplot column index\n",
    "    domains = data['Domain'].unique()\n",
    "\n",
    "    for domain in domains:\n",
    "        domain_data = data[data['Domain'] == domain]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=domain_data[\"Length Bin\"],\n",
    "                y=domain_data[\"Count\"],\n",
    "                name=domain,\n",
    "                marker_color=domain_color_map.get(domain, \"#cccccc\"),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=1, col=col\n",
    "        )\n",
    "\n",
    "# Set custom x-tick labels for all subplots\n",
    "x_ticks = ['1-3 tok', '4-6 tok', '7-12 tok', '13-200 tok']\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 1, 2, 3],\n",
    "    ticktext=x_ticks,\n",
    "    tickfont=dict(size=26),  # Set font size for all x-axis ticks\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 1, 2, 3],\n",
    "    ticktext=x_ticks,\n",
    "    tickfont=dict(size=26),  # Set font size for all x-axis ticks\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 1, 2, 3],\n",
    "    ticktext=x_ticks,\n",
    "    tickfont=dict(size=26),  # Set font size for all x-axis ticks\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Update layout and aesthetics\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    barmode='group',\n",
    "    yaxis=dict(\n",
    "        title=\"Number of Sentences\", \n",
    "        range=[0, y_max], \n",
    "        tick0=0, \n",
    "        dtick=50000,\n",
    "        title_font_size=28,  \n",
    "        tickfont=dict(size=26)\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Sentence Length Bin (Tokens)\",\n",
    "        title_font_size=28, \n",
    "        tickfont=dict(size=26)  \n",
    "    ),\n",
    "    xaxis_title_standoff=28,\n",
    "    showlegend=True,\n",
    "    template='plotly_white',\n",
    "    font=dict(size=14),\n",
    "    legend=dict(\n",
    "        title=dict(text='Model', font=dict(size=28)),\n",
    "        font=dict(size=28),\n",
    "        x=0.95, \n",
    "        y=0.95,\n",
    "        xanchor='right', \n",
    "        yanchor='top',   \n",
    "        bgcolor='rgba(255,255,255,0.8)', \n",
    "        bordercolor='gray',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    annotations=[\n",
    "\n",
    "        dict(\n",
    "            text=\"English\", font=dict(size=28), showarrow=False\n",
    "        ),\n",
    "        dict(\n",
    "            text=\"French\", font=dict(size=28), showarrow=False\n",
    "        ),\n",
    "        dict(\n",
    "            text=\"German\", font=dict(size=28), showarrow=False\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"sentence_lengths.png\", width=width_px, height=height_px, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fe26c",
   "metadata": {},
   "source": [
    "### PLOT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to assign buckets\n",
    "def assign_age_bucket(age):\n",
    "    if age <= 2:\n",
    "        return 0\n",
    "    elif age <= 5:\n",
    "        return 1\n",
    "    elif age <= 8:\n",
    "        return 2\n",
    "    elif age > 8:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_distribution(df, lang):\n",
    "    df = df.copy()\n",
    "    df = df[pd.to_numeric(df['year_of_life'], errors='coerce').notna()]\n",
    "    df['year_of_life'] = df['year_of_life'].astype(float)\n",
    "\n",
    "    df['year_binned'] = df['year_of_life'].round().astype(int)\n",
    "    dist = df['year_binned'].value_counts().sort_index()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Age (Years)': dist.index,\n",
    "        'Total # of Sentences': dist.values/len(df),\n",
    "        'Language': lang\n",
    "    })\n",
    "\n",
    "# Apply to each language\n",
    "eng_df = binned_distribution(childes_eng, 'English')\n",
    "fra_df = binned_distribution(childes_fra, 'French')\n",
    "deu_df = binned_distribution(childes_deu, 'German')\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([eng_df, fra_df, deu_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f382f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    'English': '#E63946',\n",
    "    'French': '#F4A261', \n",
    "    'German': '#1f77b4'\n",
    "}\n",
    "\n",
    "fig = px.line(\n",
    "    combined_df,\n",
    "    x='Age (Years)',\n",
    "    y='Total # of Sentences',\n",
    "    color='Language',\n",
    "    markers=transcripts_deu_childes,\n",
    "    labels={'Total # of Words': 'Total Utterances', 'Age (Years)': 'Age (Years)'},\n",
    "    color_discrete_map=color_map\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "tickvals = list(range(0, 13)) \n",
    "ticktext = [str(i) for i in range(0, 13)]\n",
    "fig.update_layout(\n",
    "    title_font_size=25,\n",
    "    xaxis=dict(\n",
    "    tickmode='array',\n",
    "    tickvals=tickvals,\n",
    "    ticktext=ticktext,\n",
    "    tickfont=dict(size=18),\n",
    "    title=dict(text='Age (Years)', font=dict(size=25)),\n",
    "    range=[-0.50, 12.03]\n",
    "),\n",
    "    yaxis=dict(\n",
    "        tickformat='.0%',\n",
    "        tickfont=dict(size=22),\n",
    "        title=dict(text='Proportion of Utterances', font=dict(size=25)),\n",
    "        range=[-0.02, 0.5]\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0.7,\n",
    "        y=0.9,\n",
    "        traceorder='normal',\n",
    "        font=dict(size=22),\n",
    "        borderwidth=1,\n",
    "        title=dict(font=dict(size=20))\n",
    "\n",
    "        \n",
    "    ),\n",
    "    template='simple_white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"utterances_by_age.png\", width=800, height=500, scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2b54c",
   "metadata": {},
   "source": [
    "### PLOT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [childes_eng, childes_fra, childes_deu]:\n",
    "    df['bucket'] = df['year_of_life_fine_grained'].apply(assign_age_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_counts = childes_eng['bucket'].value_counts().sort_index().rename('English')\n",
    "fra_counts = childes_fra['bucket'].value_counts().sort_index().rename('French')\n",
    "deu_counts = childes_deu['bucket'].value_counts().sort_index().rename('German')\n",
    "\n",
    "bucket_counts = pd.concat([eng_counts, fra_counts, deu_counts], axis=1).fillna(0).astype(int)\n",
    "bucket_counts.index.name = 'Age Bucket'\n",
    "bucket_counts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362accf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "childes_eng['language'] = 'English'\n",
    "childes_fra['language'] = 'French'\n",
    "childes_deu['language'] = 'German'\n",
    "\n",
    "# Combine all\n",
    "combined_df = pd.concat([childes_eng, childes_fra, childes_deu], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define your custom tick labels\n",
    "bucket_labels = ['0: 0–2 yrs', '1: 3–5 yrs', '2: 6–8 yrs', '3: 9+ yrs']\n",
    "custom_xticks = [\"0: 1-3 tokens\", \"1: 4-6 tokens\", \"2: 7-12 tokens\", \"3: 13-200 tokens\"]\n",
    "custom_tickvals = [0, 1, 2, 3]  # Assuming those are your bin indices\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "width_in = 6.875\n",
    "height_in = 4.0 \n",
    "dpi = 300\n",
    "\n",
    "# Convert to pixels\n",
    "width_px = int(width_in * dpi)\n",
    "height_px = int(height_in * dpi)\n",
    "\n",
    "# Loop through each language and add a bar trace\n",
    "languages = ['English', 'French', 'German']\n",
    "colors = ['#E63946', '#F4A261', '#E9C46A']\n",
    "\n",
    "for lang, color in zip(languages, colors):\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=bucket_labels,\n",
    "        y=bucket_counts[lang],\n",
    "        name=lang,\n",
    "        marker_color=color,\n",
    "        text=bucket_counts[lang],\n",
    "        textposition='auto',\n",
    "        textfont=dict(size=26)\n",
    "    ))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    xaxis=dict(\n",
    "        title=dict(\n",
    "            text='Age Buckets',\n",
    "            font=dict(size=28)\n",
    "        ),\n",
    "        tickmode='array',\n",
    "        tickvals=[0, 1, 2, 3],  # Use the correct tick values\n",
    "        ticktext=bucket_labels,\n",
    "        tickfont=dict(size=26)\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(\n",
    "            text='Number of Sentences',\n",
    "            font=dict(size=28)\n",
    "        ),\n",
    "        tickfont=dict(size=26)\n",
    "    ),\n",
    "    legend=dict(\n",
    "    title=dict(text='Language', font=dict(size=28)),\n",
    "    font=dict(size=28),\n",
    "    x=0.95,  # X position (0 to 1)\n",
    "    y=0.95,  # Y position (0 to 1)\n",
    "    xanchor='right',  # Anchor the legend's x at the right\n",
    "    yanchor='top',    # Anchor the legend's y at the top\n",
    "    bgcolor='rgba(255,255,255,0.8)',  # Optional: semi-transparent white background\n",
    "    bordercolor='gray',\n",
    "    borderwidth=1\n",
    "),\n",
    "    template='plotly_white',\n",
    "    font=dict(size=26),\n",
    "    bargap=0.2\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"sentence_counts_by_age.png\", width=width_px, height=height_px, scale=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e15a94",
   "metadata": {},
   "source": [
    "### PLOT 4 \n",
    "Keep Unigram Frequency of the Training dataset or of the full dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b56303",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_deu_childes = pd.read_csv(UNIGRAM_CHILDES_DE)\n",
    "unigram_freq_eng_childes = pd.read_csv(UNIGRAM_CHILDES_ENG)\n",
    "unigram_freq_fra_childes = pd.read_csv(UNIGRAM_CHILDES_FR)\n",
    "\n",
    "unigram_freq_deu_wiki = pd.read_csv(UNIGRAM_WIKI_DE)\n",
    "unigram_freq_eng_wiki = pd.read_csv(UNIGRAM_WIKI_ENG)\n",
    "unigram_freq_fra_wiki = pd.read_csv(UNIGRAM_WIKI_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words in each bin for a dataset\n",
    "def count_words_in_bins(data, num_bins=10):\n",
    "    bin_counts = [0] * num_bins\n",
    "    for bin_index in data['bin']:\n",
    "        bin_counts[bin_index] += 1\n",
    "    return bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd09b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign bins logarithmically\n",
    "def assign_bins_log(data, num_bins=10):\n",
    "    # Extract the frequency values\n",
    "    freq_values = data['count'].values\n",
    "\n",
    "    # Determine bin edges based on logarithmic scale\n",
    "    log_min = np.log10(min(freq_values))\n",
    "    log_max = np.log10(max(freq_values))\n",
    "    bin_edges = np.logspace(log_min, log_max, num_bins + 1)\n",
    "    \n",
    "    binned_data = np.digitize(freq_values, bin_edges) - 1\n",
    "    binned_data[binned_data == num_bins] = num_bins - 1  # Ensure values outside range go to the last bin\n",
    "\n",
    "    return binned_data, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add the 'bin' column to each dataset\n",
    "def add_bin_column(data, num_bins=10):\n",
    "    binned_data, bin_edges = assign_bins_log(data, num_bins)\n",
    "    data['bin'] = binned_data\n",
    "    return data, bin_edges\n",
    "\n",
    "# Apply to all datasets\n",
    "unigram_freq_deu_childes, bin_edges_deu_childes = add_bin_column(unigram_freq_deu_childes)\n",
    "unigram_freq_eng_childes, bin_edges_eng_childes = add_bin_column(unigram_freq_eng_childes)\n",
    "unigram_freq_fra_childes, bin_edges_fra_childes = add_bin_column(unigram_freq_fra_childes)\n",
    "\n",
    "unigram_freq_deu_wiki, bin_edges_deu_wiki = add_bin_column(unigram_freq_deu_wiki)\n",
    "unigram_freq_eng_wiki, bin_edges_eng_wiki = add_bin_column(unigram_freq_eng_wiki)\n",
    "unigram_freq_fra_wiki, bin_edges_fra_wiki = add_bin_column(unigram_freq_fra_wiki)\n",
    "\n",
    "# Example: Check the new 'bin' column in one of the datasets\n",
    "print(unigram_freq_deu_childes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be483377",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_in = 6.875\n",
    "height_in = 4.0 \n",
    "dpi = 300\n",
    "\n",
    "# Convert to pixels\n",
    "width_px = int(width_in * dpi)\n",
    "height_px = int(height_in * dpi)\n",
    "\n",
    "# Function to count words in each bin for a dataset\n",
    "def count_words_in_bins(data, num_bins=10):\n",
    "    bin_counts = [0] * num_bins\n",
    "    for bin_index in data['bin']:\n",
    "        bin_counts[bin_index] += 1\n",
    "    return bin_counts\n",
    "\n",
    "# Function to create a vertical bar plot for each language\n",
    "def create_subplot_for_language(fig, data_childes, data_wiki, language, row, col, num_bins=10):\n",
    "    # Count words in bins for both datasets\n",
    "    bin_counts_childes = count_words_in_bins(data_childes, num_bins)\n",
    "    bin_counts_wiki = count_words_in_bins(data_wiki, num_bins)\n",
    "    \n",
    "    # Create horizontal bar plot (side-by-side bars for each bin)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=np.arange(num_bins),  # Bin numbers (now on y-axis)\n",
    "            x=bin_counts_childes,  # Word count for Childes dataset (now on x-axis)\n",
    "            name='CHILDES',\n",
    "            marker_color='#e74c3c',\n",
    "            orientation='h',  # Make it horizontal\n",
    "            showlegend=(row == 1 and col == 1),\n",
    "            text=bin_counts_childes,\n",
    "            textposition='auto',\n",
    "            textfont=dict(size=26)\n",
    "        ), row=row, col=col\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=np.arange(num_bins),\n",
    "            x=bin_counts_wiki,\n",
    "            name='Wikipedia',\n",
    "            marker_color='#f1c40f',\n",
    "            orientation='h',\n",
    "            showlegend=(row == 1 and col == 1),\n",
    "            text=bin_counts_wiki,\n",
    "            textposition='auto',\n",
    "            textfont=dict(size=26)\n",
    "        ), row=row, col=col\n",
    "    )\n",
    "\n",
    "# Create the subplots for the three languages\n",
    "languages = ['eng', 'fra', 'deu']\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3, \n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=[\"English\", \"French\", \"German\"],\n",
    "    horizontal_spacing=0.05,  # Reduce space between subplots\n",
    "    vertical_spacing=0.2  # Adjust vertical spacing between rows if needed\n",
    ")\n",
    "\n",
    "# Loop through each language and create a subplot\n",
    "for i, language in enumerate(languages):\n",
    "    if language == 'eng':\n",
    "        data_childes = unigram_freq_eng_childes\n",
    "        data_wiki = unigram_freq_eng_wiki\n",
    "    elif language == 'fra':\n",
    "        data_childes = unigram_freq_fra_childes\n",
    "        data_wiki = unigram_freq_fra_wiki\n",
    "    elif language == 'deu':\n",
    "        data_childes = unigram_freq_deu_childes\n",
    "        data_wiki = unigram_freq_deu_wiki\n",
    "    \n",
    "    # Create a subplot for this language\n",
    "    create_subplot_for_language(fig, data_childes, data_wiki, language, row=1, col=i + 1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,  # You can make it taller now if needed\n",
    "    width=1000,\n",
    "    barmode='group',\n",
    "    xaxis_title=\"Word Count\",  # Swapped!\n",
    "    yaxis_title=\"Log Frequency Bin\",\n",
    "    showlegend=True,\n",
    "    template='plotly_white',\n",
    "    font=dict(size=28),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"English\", font=dict(size=28), showarrow=False\n",
    "        ),\n",
    "        dict(\n",
    "            text=\"French\", font=dict(size=28), showarrow=False\n",
    "        ),\n",
    "        dict(\n",
    "            text=\"German\", font=dict(size=28), showarrow=False\n",
    "        )\n",
    "    ],\n",
    "    legend=dict(\n",
    "        title=\"Model\",  # Add a title for the legend\n",
    "        title_font=dict(size=28),  # Set font size for legend title\n",
    "        font=dict(size=26),\n",
    "        bordercolor='gray',\n",
    "        borderwidth=1,\n",
    "        x=0.9,  # Move legend left (default is 1.0, right-aligned)\n",
    "        y=0.97   # Keep legend vertically aligned at top\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    fig.update_yaxes(\n",
    "        tickvals=np.arange(0, 10),\n",
    "        ticktext=[str(i) for i in range(10)],\n",
    "        tickfont=dict(size=26),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "fig.write_image(\"log_frequency_bins.png\", width=width_px, height=height_px, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d05eaf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
